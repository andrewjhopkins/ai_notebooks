{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5dbbb3-1070-4be8-bc29-6e3905dcd181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05c1846f-508d-4079-a8e5-2d2d20bcd1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# attention example\n",
    "\n",
    "# batch - dimension that represents number of samples being processed simultaneously in one forward pass\n",
    "# time (also called sequence length) - number of time steps (or tokens) in the input sequence\n",
    "# channels (also called embedding size or features) - number of features or dimesionality of each token representation (dimension of embeddings)\n",
    "    # corresponds to the size of hidden layers in the transformer\n",
    "batch_size, seq_len, embedding_dim = 4, 8, 32\n",
    "x = torch.randn(batch_size, seq_len, embedding_dim) \n",
    "\n",
    "# single head perform self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "query = nn.Linear(embedding_dim, head_size, bias=False) \n",
    "value = nn.Linear(embedding_dim, head_size, bias=False)\n",
    "k = key(x) # [4, 8, 16]\n",
    "q = query(x) # [4, 8, 16]\n",
    "\n",
    "# Q matmul K^T\n",
    "\n",
    "raw_attention = q @ k.transpose(-2, -1)\n",
    "\n",
    "# scaled attention 1/sqrt(head_size)\n",
    "raw_attention = raw_attention * head_size**-0.5\n",
    "\n",
    "# add masking so later does not influence earlier\n",
    "tril = torch.tril(torch.ones(seq_len, seq_len))\n",
    "masked_attention = raw_attention.masked_fill(tril == 0, float(\"-inf\")) # any characters after the sequence set to -inf\n",
    "\n",
    "# apply softmax\n",
    "attention_weights = F.softmax(masked_attention, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "attention = attention_weights @ v\n",
    "\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a8cc3396-4c20-4848-8e07-37a8c87fd83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb6040d45f0>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper params\n",
    "batch_size = 16 # amount of independent sequences processed in parallel\n",
    "block_size = 32 # max content length for predictions\n",
    "\n",
    "max_iterations = 50000\n",
    "evaluation_interval = 1000\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "evaluation_iterations = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c1d122a-d6b0-446d-9f68-b54e51b71c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a417b53a-6df7-466b-8de5-080dc04c0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tokenizer\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_token = {}\n",
    "token_to_char = {}\n",
    "for i in range(len(chars)):\n",
    "    char_to_token[chars[i]] = i\n",
    "    token_to_char[i] = chars[i]\n",
    "\n",
    "encode = lambda word: [char_to_token[char] for char in word]\n",
    "decode = lambda tokens: [token_to_char[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e1bf4b3-a935-4fa0-abdf-732c11ae8608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'e', 's', 't']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82b80fdf-e022-49c3-89df-8c59887c0402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n"
     ]
    }
   ],
   "source": [
    "# load data and tokenize\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a06cd978-c7c1-4e09-a27b-1a9247e87648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854 111540\n"
     ]
    }
   ],
   "source": [
    "# split into training and validation\n",
    "n = int(len(data) * 0.9)\n",
    "data_train = data[:n]\n",
    "data_test = data[n:]\n",
    "\n",
    "print(len(data_train), len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1312f9d9-99b7-4af4-812e-3a8df77a36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batches of data\n",
    "def get_batch(split):\n",
    "    data = data_train if split == \"train\" else data_test\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ac3784bf-1cc8-43ca-9dcb-2dd17a650c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embedding_dim = x.shape\n",
    "        k = self.key(x) # (batch_size, seq_len, embedding_dim)\n",
    "        q = self.query(x) # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # compute attention scores\n",
    "        # Q matmul K^T\n",
    "        raw_attention = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # scaled attention 1/sqrt(head_size)\n",
    "        raw_attention = raw_attention * head_size**-0.5 # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # add masking so later does not influence earlier\n",
    "        tril = torch.tril(torch.ones(seq_len, seq_len))\n",
    "        masked_attention = raw_attention.masked_fill(self.tril[:seq_len, :seq_len] == 0, float(\"-inf\")) # any characters after the sequence set to -inf\n",
    "\n",
    "        # apply softmax\n",
    "        attention_weights = F.softmax(masked_attention, dim=-1)  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        # perform weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        attention = attention_weights @ v # (batch_size, seq_len, embedding_dim)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "19947f6b-54bc-4c2d-adda-4fc441d165bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # multiple heads of self attention in parallel\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # combine all attention\n",
    "        out = torch.cat([attentionHead(x) for attentionHead in self.heads], dim=-1)\n",
    "        # modify embeddings with combined attention\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a4c50f10-2cef-4f2e-9b69-2379a1ef3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    # linear layer followed by non linearity\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "            return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4f3d5369-b51f-4477-a5cf-4d5bd3c6118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # transformer block: communication followed by computation\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "227b1b09-3c78-4df1-89e8-90d402515840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple BigramLanguageModel\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # each token directly reads off logits for next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        batch_size, seq_len = idx.shape\n",
    "\n",
    "        # idx and targets are both (batch_size, seq_len) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (batch_size, seq_len, emb_dimension)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(seq_len, device=device)) # (seq_len, emb_dimension)\n",
    "        x = tok_emb + pos_emb # (batch_size, seq_len, emb_dimension)\n",
    "        x = self.blocks(x) # (batch_size, seq_len, emb_dimension)\n",
    "        x = self.ln_f(x) # (batch_size, seq_len, emb_dimension)\n",
    "        logits = self.lm_head(x) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, seq_len, emb_dimension = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, emb_dimension)\n",
    "            targets = targets.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (batch_size, seq_len) array of indices in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, emb_dimension)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (batch_size, emb_dimension)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (batch_size, seq_len + 1)\n",
    "        return idx\n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e2949c37-5fb4-4b2e-b624-dca67c628419",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9dbc0310-21b7-48a3-928a-dcb998737011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209729 parameters\n"
     ]
    }
   ],
   "source": [
    "# prin the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()), \"parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c302825a-415f-4a00-ae6f-c58b8ba3c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5d98ff2b-f727-4d5d-8132-032051d21d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(evaluation_iterations)\n",
    "        for k in range(evaluation_iterations):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9cba448c-4334-4223-a520-4a3be94d1eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.6095, val loss 1.7770\n",
      "step 1000: train loss 1.5246, val loss 1.7094\n",
      "step 2000: train loss 1.5233, val loss 1.7182\n",
      "step 3000: train loss 1.5094, val loss 1.6947\n",
      "step 4000: train loss 1.5083, val loss 1.6935\n",
      "step 5000: train loss 1.4989, val loss 1.6733\n",
      "step 6000: train loss 1.4995, val loss 1.6758\n",
      "step 7000: train loss 1.4798, val loss 1.6685\n",
      "step 8000: train loss 1.4846, val loss 1.6762\n",
      "step 9000: train loss 1.4719, val loss 1.6460\n",
      "step 10000: train loss 1.4718, val loss 1.6702\n",
      "step 11000: train loss 1.4496, val loss 1.6386\n",
      "step 12000: train loss 1.4553, val loss 1.6546\n",
      "step 13000: train loss 1.4534, val loss 1.6458\n",
      "step 14000: train loss 1.4594, val loss 1.6474\n",
      "step 15000: train loss 1.4449, val loss 1.6457\n",
      "step 16000: train loss 1.4459, val loss 1.6498\n",
      "step 17000: train loss 1.4396, val loss 1.6465\n",
      "step 18000: train loss 1.4349, val loss 1.6425\n",
      "step 19000: train loss 1.4217, val loss 1.6309\n",
      "step 20000: train loss 1.4353, val loss 1.6442\n",
      "step 21000: train loss 1.4232, val loss 1.6451\n",
      "step 22000: train loss 1.4247, val loss 1.6523\n",
      "step 23000: train loss 1.4211, val loss 1.6431\n",
      "step 24000: train loss 1.4189, val loss 1.6329\n",
      "step 25000: train loss 1.4278, val loss 1.6460\n",
      "step 26000: train loss 1.4220, val loss 1.6297\n",
      "step 27000: train loss 1.4113, val loss 1.6414\n",
      "step 28000: train loss 1.4185, val loss 1.6254\n",
      "step 29000: train loss 1.4105, val loss 1.6196\n",
      "step 30000: train loss 1.4080, val loss 1.6244\n",
      "step 31000: train loss 1.4129, val loss 1.6218\n",
      "step 32000: train loss 1.4017, val loss 1.6347\n",
      "step 33000: train loss 1.4053, val loss 1.6261\n",
      "step 34000: train loss 1.4074, val loss 1.6227\n",
      "step 35000: train loss 1.3935, val loss 1.6392\n",
      "step 36000: train loss 1.3947, val loss 1.6173\n",
      "step 37000: train loss 1.4017, val loss 1.6273\n",
      "step 38000: train loss 1.3960, val loss 1.6091\n",
      "step 39000: train loss 1.4006, val loss 1.6202\n",
      "step 40000: train loss 1.3914, val loss 1.6248\n",
      "step 41000: train loss 1.4041, val loss 1.6361\n",
      "step 42000: train loss 1.3922, val loss 1.6188\n",
      "step 43000: train loss 1.3892, val loss 1.6008\n",
      "step 44000: train loss 1.3893, val loss 1.6033\n",
      "step 45000: train loss 1.3839, val loss 1.6112\n",
      "step 46000: train loss 1.3815, val loss 1.6227\n",
      "step 47000: train loss 1.3899, val loss 1.6057\n",
      "step 48000: train loss 1.3829, val loss 1.5960\n",
      "step 49000: train loss 1.3858, val loss 1.6290\n",
      "step 49999: train loss 1.3859, val loss 1.6196\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iterations):\n",
    "    # every once in a while evalaute loss on train and val sets\n",
    "    if iter % evaluation_interval == 0 or iter == max_iterations - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "23230dc4-58d5-4196-b66e-e18d1a2cafa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Strude inform my poor.\n",
      "Though I have real'd your mastrans; and knees you requirest\n",
      "Have as many hurts of George,\n",
      "Farewill's villain in the jury wind upon,\n",
      "At chance thee, and hence to the denger\n",
      "Lies are, and they and lick me: or God,\n",
      "Commendance, tyll have word'st title, gives! Romeo!\n",
      "\n",
      "COMINIUS:\n",
      "I would indeed there is your lifest gayss\n",
      "From a trait? well, therefore do with childimb and I\n",
      "Happily now, and becomes of us!\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "I hard and my untroop,\n",
      "Being and we shorter I mear-font he with themselves\n",
      "from the bribed, my eands that have\n",
      "will be indequer me to the died upon\n",
      "Thereables in content, and I do long of with bed inderious dear.\n",
      "\n",
      "First Murderer:\n",
      "Not all day and behave! Lave your own Romeo's arm farewell'd.\n",
      "\n",
      "Third Servingman:\n",
      "Where will west, my care\n",
      "Marcius ward, thy mirtion. For el there not should be long of the king to-n,\n",
      "From it on. We hadeful song down,\n",
      "By live, to those hath with the morning to wredged\n",
      "For you; Buckingham, wearing that.\n",
      "\n",
      "KING HENRY VI:\n",
      "Ay, by rite thou but by friend!\n",
      "\n",
      "Musicile:\n",
      "The have time grows up and Kate Richard,\n",
      "That nexts well arm as you-back banish,\n",
      "Say shall age may kneel\n",
      "To quenting about of war; answer, and seems are our looks?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "This a mode-stand delight with that cause of you,\n",
      "Conjuies of all me, promine and treail,\n",
      "If in aslest that absence to her appearing for strongs and worth,\n",
      "as is he was jurit'd honest.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "What can I lid to death.\n",
      "\n",
      "Shepherd:\n",
      "\n",
      "LEONTES:\n",
      "Warrant it of father's lord.\n",
      "\n",
      "COMINIUS:\n",
      "What I had many stroke of mine; let me as I sonder.\n",
      "Withily! and for heaven men, that's a name\n",
      "And on the pity.\n",
      "\n",
      "KING RICHARD III:\n",
      "Let joy them when thou hast a least.\n",
      "\n",
      "FRIAR PERLE:\n",
      "Here's thy shorts of the offended no putse that to appreove no.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "And now at nay.' Alack: he, by John.\n",
      "Country's Northumberland,\n",
      "of all audicter'd; Say to How to beauty.\n",
      "\n",
      "COMINIUS:\n",
      "Thou art Saint A\n",
      "Move my sovered compets of her and gentleman.\n",
      "\n",
      "DUKE OF GAUNT:\n",
      "Thus madam: they stand to come? Penu\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([[char_to_token[char] for char in \"ROMEO:\"]], dtype=torch.long, device=device)\n",
    "print(\"\".join(decode(model.generate(context, max_new_tokens=2000)[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934eab3-4367-4df5-adbf-ceac6d5d5830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
