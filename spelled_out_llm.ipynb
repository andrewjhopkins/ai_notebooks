{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b5dbbb3-1070-4be8-bc29-6e3905dcd181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c1846f-508d-4079-a8e5-2d2d20bcd1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# attention example\n",
    "\n",
    "# batch_size - dimension that represents number of samples being processed simultaneously in one forward pass\n",
    "# seq_len (also called time) - number of time steps (or tokens) in the input sequence\n",
    "# embedding_dim (also called channels or features) - number of features or dimesionality of each token representation (dimension of embeddings)\n",
    "    # corresponds to the size of hidden layers in the transformer\n",
    "batch_size, seq_len, emb_dimension = 4, 8, 32\n",
    "x = torch.randn(batch_size, seq_len, emb_dimension) \n",
    "\n",
    "# single head perform self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(emb_dimension, head_size, bias=False)\n",
    "query = nn.Linear(emb_dimension, head_size, bias=False) \n",
    "value = nn.Linear(emb_dimension, head_size, bias=False)\n",
    "# keys and queries\n",
    "k = key(x) # [4, 8, 16]\n",
    "q = query(x) # [4, 8, 16]\n",
    "\n",
    "# Q matmul K^T\n",
    "\n",
    "raw_attention = q @ k.transpose(-2, -1)\n",
    "\n",
    "# scaled attention 1/sqrt(head_size)\n",
    "raw_attention = raw_attention * head_size**-0.5\n",
    "\n",
    "# add masking so later does not influence earlier\n",
    "tril = torch.tril(torch.ones(seq_len, seq_len))\n",
    "masked_attention = raw_attention.masked_fill(tril == 0, float(\"-inf\")) # any characters after the sequence set to -inf\n",
    "\n",
    "# apply softmax\n",
    "attention_weights = F.softmax(masked_attention, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "attention = attention_weights @ v\n",
    "\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8cc3396-4c20-4848-8e07-37a8c87fd83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0a82250670>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper params\n",
    "batch_size = 16 # amount of independent sequences processed in parallel\n",
    "block_size = 32 # max content length for predictions\n",
    "\n",
    "max_iterations = 5000\n",
    "evaluation_interval = 1000\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "evaluation_iterations = 200\n",
    "# embedding dimensions\n",
    "n_embd = 64\n",
    "# number of heads\n",
    "n_head = 4\n",
    "# number of layers\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c1d122a-d6b0-446d-9f68-b54e51b71c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a417b53a-6df7-466b-8de5-080dc04c0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a tokenizer\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_token = {}\n",
    "token_to_char = {}\n",
    "for i in range(len(chars)):\n",
    "    char_to_token[chars[i]] = i\n",
    "    token_to_char[i] = chars[i]\n",
    "\n",
    "encode = lambda word: [char_to_token[char] for char in word]\n",
    "decode = lambda tokens: [token_to_char[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e1bf4b3-a935-4fa0-abdf-732c11ae8608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'e', 's', 't']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b80fdf-e022-49c3-89df-8c59887c0402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n"
     ]
    }
   ],
   "source": [
    "# load data and tokenize\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a06cd978-c7c1-4e09-a27b-1a9247e87648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854 111540\n"
     ]
    }
   ],
   "source": [
    "# split into training and validation\n",
    "n = int(len(data) * 0.9)\n",
    "data_train = data[:n]\n",
    "data_test = data[n:]\n",
    "\n",
    "print(len(data_train), len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1312f9d9-99b7-4af4-812e-3a8df77a36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate batches of data\n",
    "def get_batch(split):\n",
    "    data = data_train if split == \"train\" else data_test\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac3784bf-1cc8-43ca-9dcb-2dd17a650c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single attention head\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, emb_dimension = x.shape\n",
    "        k = self.key(x) # (batch_size, seq_len, emb_dimension)\n",
    "        q = self.query(x) # (batch_size, seq_len, emb_dimension)\n",
    "        \n",
    "        # compute attention scores\n",
    "        # Q matmul K^T\n",
    "        raw_attention = q @ k.transpose(-2, -1)\n",
    "\n",
    "        # scaled attention 1/sqrt(head_size)\n",
    "        raw_attention = raw_attention * head_size**-0.5 # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # add masking so later does not influence earlier\n",
    "        tril = torch.tril(torch.ones(seq_len, seq_len))\n",
    "        masked_attention = raw_attention.masked_fill(self.tril[:seq_len, :seq_len] == 0, float(\"-inf\")) # any characters after the sequence set to -inf\n",
    "\n",
    "        # apply softmax\n",
    "        attention_weights = F.softmax(masked_attention, dim=-1)  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        # perform weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        attention = attention_weights @ v # (batch_size, seq_len, emb_dimension)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19947f6b-54bc-4c2d-adda-4fc441d165bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # multiple heads of self attention in parallel\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # combine all attention\n",
    "        out = torch.cat([attentionHead(x) for attentionHead in self.heads], dim=-1)\n",
    "        # modify embeddings with combined attention\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4c50f10-2cef-4f2e-9b69-2379a1ef3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    # two linear transformations with ReLU in between\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "            return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f3d5369-b51f-4477-a5cf-4d5bd3c6118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # transformer block: communication followed by computation\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "227b1b09-3c78-4df1-89e8-90d402515840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple BigramLanguageModel\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # each token directly reads off logits for next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        # transformer blocks equivalent to number of layers\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # final layer norm\n",
    "        self.ln_f = nn.LayerNorm(n_embd) \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        batch_size, seq_len = idx.shape\n",
    "\n",
    "        # idx and targets are both (batch_size, seq_len) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (batch_size, seq_len, emb_dimension)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(seq_len, device=device)) # (seq_len, emb_dimension)\n",
    "        x = tok_emb + pos_emb # (batch_size, seq_len, emb_dimension)\n",
    "        x = self.blocks(x) # (batch_size, seq_len, emb_dimension)\n",
    "        x = self.ln_f(x) # (batch_size, seq_len, emb_dimension)\n",
    "        logits = self.lm_head(x) # (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, seq_len, emb_dimension = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, emb_dimension)\n",
    "            targets = targets.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (batch_size, seq_len) array of indices in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, emb_dimension)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (batch_size, emb_dimension)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (batch_size, seq_len + 1)\n",
    "        return idx\n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2949c37-5fb4-4b2e-b624-dca67c628419",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dbc0310-21b7-48a3-928a-dcb998737011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209729 parameters\n"
     ]
    }
   ],
   "source": [
    "# prin the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters()), \"parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c302825a-415f-4a00-ae6f-c58b8ba3c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d98ff2b-f727-4d5d-8132-032051d21d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(evaluation_iterations)\n",
    "        for k in range(evaluation_iterations):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cba448c-4334-4223-a520-4a3be94d1eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4109, val loss 4.4016\n",
      "step 1000: train loss 2.0616, val loss 2.1039\n",
      "step 2000: train loss 1.8601, val loss 1.9621\n",
      "step 3000: train loss 1.7624, val loss 1.8959\n",
      "step 4000: train loss 1.7135, val loss 1.8693\n",
      "step 4999: train loss 1.6592, val loss 1.8031\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iterations):\n",
    "    # every once in a while evalaute loss on train and val sets\n",
    "    if iter % evaluation_interval == 0 or iter == max_iterations - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23230dc4-58d5-4196-b66e-e18d1a2cafa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Neave need eneough, thy ariveds\n",
      "Smeof indempt and pasefore of my stame their it\n",
      "And a shept you would, to unatule mude.\n",
      "\n",
      "FLUEEN:\n",
      "Lest, cotrith:\n",
      "Who should your foiles; go, onemain that he seepor take not beleing you the fears tithe done! I have m ere off.\n",
      "\n",
      "CLAUDOREN:\n",
      "Well,\n",
      "Godsaloress of daying the chadd that's 'twend yixeding brinks duve.\n",
      "\n",
      "WAMPNEY:\n",
      "Ay, by lelf.\n",
      "\n",
      "WESTIO:\n",
      "No't, madam; York your Fath-in bil her a than mes\n",
      "And any mean! with eak this bisher,\n",
      "And Ladone a desenting, what; I fair werppe.\n",
      "\n",
      "AUTOLUCESTER:\n",
      "\n",
      "ANGELO:\n",
      "\n",
      "pleter As light folt;\n",
      "And you boot marks you hadd most. Be sonscreanceman in eyes\n",
      "The brajensonens upon this? which thempery inds cizivel a backs,\n",
      "There not she faine with ning our haddes to my's stay inlet etward, do't mord'sts larminatoness, would as like is frie,\n",
      "Ke's own takes be son! is be it ouf a clonaceof to sits? when hurn, but veight My calf bust dousburt gon bleever leads?\n",
      "\n",
      "Than Thont Rife BOineand ay?\n",
      "I am to upon thy curnely brothy to bold.\n",
      "Make warsh, much as love in this marges.\n",
      "\n",
      "CLORIO:\n",
      "I disse ower in thee; it forgeds,\n",
      "The cad nridmond your done of he was outrean's one.\n",
      "\n",
      "NRAPETELIZA:\n",
      "You grave, theat you, if hand upon,\n",
      "and the frie worth the bestand off, on mark off the mery puty.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Our by joy; must, let's a fouts, and of all on cricess' it forfend\n",
      "Frither, Relad pasticele on in that once's\n",
      "Of lit; helessbard of eizen? broke you duke with in bother\n",
      "For much inflived the shove post on.\n",
      "\n",
      "DUKE VINCENTINTIO:\n",
      "What, I have henjest ounderead; unf the sutch of a foend in princer-faceiviey\n",
      "Wenteves Rome pill it, live in Befince.\n",
      "I need you revent, thence! I diserate\n",
      "Suffettersed'd in thempunest flean obe the worthrow; Ieneve thysel.\n",
      "I have in lamok, a than sin argainst son breany,\n",
      "And seeque, the world of recomion!'I' wound could takes: nursely face\n",
      "Tondly matter left,\n",
      "Lady. He my lone was wlaint\n",
      "and most and fourt.\n",
      "\n",
      "DUKE VINCENTILLA:\n",
      "So she gason to much words\n",
      "Dirispote of be picksion: some and drevant:\n",
      "And thord, gore?\n",
      "Augh t\n"
     ]
    }
   ],
   "source": [
    "# start with prompt \"ROMEO:\"\n",
    "context = torch.tensor([[char_to_token[char] for char in \"ROMEO:\"]], dtype=torch.long, device=device)\n",
    "print(\"\".join(decode(model.generate(context, max_new_tokens=2000)[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934eab3-4367-4df5-adbf-ceac6d5d5830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
